---
title: WaNN generic
tags: [ "GoogleDocs", "Meta" ]
---

## NN at a particular scale

* Trainable and non-trainable variables
* Particular loss function
* Learns and improves its abilities.
* NN is developing models
	* Learning of Distributions, Bayesian Learning
		* Environment
		* Model of Self - Consciousness
		* Action of Self on Environment
		* Planning (Possible words)
		* Counterfactuals

## NN goes through phase transition

* Specialised connectome appears from fully connected network
* Trainable variables are separated  into temporal classes. Slow and fast changing. New time-scale appears.
* Information Processing Units (IPUs) are formed spatially - clustering.
	* Densely connected 
	* Common slow-changing variables in the cluster. 
* Communication channels between IPUs are established
	* Sparse connectome compared to dense connectome inside the cluster
* Boundaries appear, new IPUs  become persistent and are “preserving their identities”
* Specialization of IPUs. 
* New space scale appears (dimensionality, topology, metric).
	* Locality appears
* IPUs can be considered neurons in the NN at the next scale
* Loss function emerges for a new NN???
	* Components: Model quality, Surprise, Discovery,

## NN at the next scale starts learning

* Can we abstract away from fundamental neurons and all lower-scale IPUs while we are analysing some level? 
* Can we understand this level based on the knowledge of its IPUs?

[Google Doc](https://docs.google.com/document/d/1shWs9VT3kevOSZp4xTfSFJGHMCVe0KVoby6LqMMKNh0/edit)
