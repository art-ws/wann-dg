---
title: WaNN generic
tags: [ "GoogleDocs", "Meta" ]
---

[[Terms/WaNN|WaNN]] conspect.

## NN at a particular scale

* Trainable and non-trainable variables
* Particular [[Terms/NN/Loss function|loss function]]
* Learns and improves its abilities.
* NN is developing models
	* Learning of Distributions, Bayesian Learning
		* [[Terms/NN/Environment|Environment]]
		* Model of Self - [[Terms/Consciousness|Consciousness]]
		* Action of Self on [[Terms/NN/Environment|Environment]]
		* Planning (Possible words)
		* Counterfactuals

## NN goes through [[Terms/Phase transition|phase transition]]

* Specialized [[Terms/Connectome|connectome]] appears from fully connected network
* Trainable variables are separated  into temporal classes. Slow and fast changing. New time-scale appears.
* [[Terms/NN/IPU|Information Processing Units]] (IPUs) are formed spatially - clustering.
	* Densely connected 
	* Common slow-changing variables in the cluster. 
* Communication channels between IPUs are established
	* Sparse [[Terms/Connectome|connectome]] compared to dense connectome inside the cluster
* Boundaries appear, new IPUs  become persistent and are “preserving their identities”
* Specialization of IPUs. 
* New space scale appears (dimensionality, topology, metric).
	* Locality appears
* IPUs can be considered neurons in the NN at the next scale
* [[Terms/NN/Loss function|Loss function]] emerges for a new [[Terms/NN/NN|NN]]???
	* Components: Model quality, Surprise, Discovery,

## [[Terms/NN/NN|NN]] at the next scale starts learning

* Can we abstract away from [[Terms/NN/Fundamental Neurons|fundamental neurons]] and all lower-scale IPUs while we are analyzing some level? 
* Can we understand this level based on the knowledge of its IPUs?

[Google Doc](https://docs.google.com/document/d/1shWs9VT3kevOSZp4xTfSFJGHMCVe0KVoby6LqMMKNh0/edit)
